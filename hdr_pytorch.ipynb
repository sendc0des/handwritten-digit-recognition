{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f1bd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74bf355d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                # Convert images to PyTorch tensors (shape: [1,28,28], values in [0,1])\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize: (x - 0.5)/0.5 → values in [-1,1]\n",
    "])\n",
    "\n",
    "# Load MNIST training and test datasets\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Wrap them in DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0579e3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DigitClassifier(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DigitClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitClassifier, self).__init__()\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(28*28, 128)   # fully connected: 784 → 128\n",
    "        self.fc2 = nn.Linear(128, 64)      # fully connected: 128 → 64\n",
    "        self.fc3 = nn.Linear(64, 10)       # fully connected: 64 → 10 (digits 0–9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the image: from [batch,1,28,28] → [batch,784]\n",
    "        x = x.view(-1, 28*28)\n",
    "        # Pass through layers with ReLU activations\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        # Final layer: raw class scores (logits)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create the model and move it to device (CPU/GPU)\n",
    "model = DigitClassifier().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edbbbf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: compares logits (from model) with true labels\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: Adam optimizer updates model weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b9d582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [0], Loss: 0.0280\n",
      "Epoch [1/15], Step [100], Loss: 0.0822\n",
      "Epoch [1/15], Step [200], Loss: 0.1229\n",
      "Epoch [1/15], Step [300], Loss: 0.0796\n",
      "Epoch [1/15], Step [400], Loss: 0.1582\n",
      "Epoch [1/15], Step [500], Loss: 0.1843\n",
      "Epoch [1/15], Step [600], Loss: 0.0828\n",
      "Epoch [1/15], Step [700], Loss: 0.1504\n",
      "Epoch [1/15], Step [800], Loss: 0.0528\n",
      "Epoch [1/15], Step [900], Loss: 0.0191\n",
      "Epoch [2/15], Step [0], Loss: 0.0535\n",
      "Epoch [2/15], Step [100], Loss: 0.0392\n",
      "Epoch [2/15], Step [200], Loss: 0.0302\n",
      "Epoch [2/15], Step [300], Loss: 0.0744\n",
      "Epoch [2/15], Step [400], Loss: 0.1135\n",
      "Epoch [2/15], Step [500], Loss: 0.0461\n",
      "Epoch [2/15], Step [600], Loss: 0.3819\n",
      "Epoch [2/15], Step [700], Loss: 0.1369\n",
      "Epoch [2/15], Step [800], Loss: 0.0447\n",
      "Epoch [2/15], Step [900], Loss: 0.0154\n",
      "Epoch [3/15], Step [0], Loss: 0.0524\n",
      "Epoch [3/15], Step [100], Loss: 0.0059\n",
      "Epoch [3/15], Step [200], Loss: 0.1439\n",
      "Epoch [3/15], Step [300], Loss: 0.0843\n",
      "Epoch [3/15], Step [400], Loss: 0.0312\n",
      "Epoch [3/15], Step [500], Loss: 0.0225\n",
      "Epoch [3/15], Step [600], Loss: 0.1782\n",
      "Epoch [3/15], Step [700], Loss: 0.0473\n",
      "Epoch [3/15], Step [800], Loss: 0.0413\n",
      "Epoch [3/15], Step [900], Loss: 0.0265\n",
      "Epoch [4/15], Step [0], Loss: 0.0741\n",
      "Epoch [4/15], Step [100], Loss: 0.0116\n",
      "Epoch [4/15], Step [200], Loss: 0.1493\n",
      "Epoch [4/15], Step [300], Loss: 0.0545\n",
      "Epoch [4/15], Step [400], Loss: 0.1133\n",
      "Epoch [4/15], Step [500], Loss: 0.0565\n",
      "Epoch [4/15], Step [600], Loss: 0.1391\n",
      "Epoch [4/15], Step [700], Loss: 0.1409\n",
      "Epoch [4/15], Step [800], Loss: 0.1015\n",
      "Epoch [4/15], Step [900], Loss: 0.0221\n",
      "Epoch [5/15], Step [0], Loss: 0.0278\n",
      "Epoch [5/15], Step [100], Loss: 0.0090\n",
      "Epoch [5/15], Step [200], Loss: 0.0590\n",
      "Epoch [5/15], Step [300], Loss: 0.0802\n",
      "Epoch [5/15], Step [400], Loss: 0.0556\n",
      "Epoch [5/15], Step [500], Loss: 0.0817\n",
      "Epoch [5/15], Step [600], Loss: 0.0815\n",
      "Epoch [5/15], Step [700], Loss: 0.2328\n",
      "Epoch [5/15], Step [800], Loss: 0.2258\n",
      "Epoch [5/15], Step [900], Loss: 0.0220\n",
      "Epoch [6/15], Step [0], Loss: 0.0208\n",
      "Epoch [6/15], Step [100], Loss: 0.0144\n",
      "Epoch [6/15], Step [200], Loss: 0.0382\n",
      "Epoch [6/15], Step [300], Loss: 0.0492\n",
      "Epoch [6/15], Step [400], Loss: 0.0255\n",
      "Epoch [6/15], Step [500], Loss: 0.0640\n",
      "Epoch [6/15], Step [600], Loss: 0.0901\n",
      "Epoch [6/15], Step [700], Loss: 0.1357\n",
      "Epoch [6/15], Step [800], Loss: 0.0481\n",
      "Epoch [6/15], Step [900], Loss: 0.0187\n",
      "Epoch [7/15], Step [0], Loss: 0.0581\n",
      "Epoch [7/15], Step [100], Loss: 0.0581\n",
      "Epoch [7/15], Step [200], Loss: 0.0193\n",
      "Epoch [7/15], Step [300], Loss: 0.0218\n",
      "Epoch [7/15], Step [400], Loss: 0.0659\n",
      "Epoch [7/15], Step [500], Loss: 0.0588\n",
      "Epoch [7/15], Step [600], Loss: 0.0265\n",
      "Epoch [7/15], Step [700], Loss: 0.0034\n",
      "Epoch [7/15], Step [800], Loss: 0.0207\n",
      "Epoch [7/15], Step [900], Loss: 0.0046\n",
      "Epoch [8/15], Step [0], Loss: 0.0137\n",
      "Epoch [8/15], Step [100], Loss: 0.1541\n",
      "Epoch [8/15], Step [200], Loss: 0.0091\n",
      "Epoch [8/15], Step [300], Loss: 0.0534\n",
      "Epoch [8/15], Step [400], Loss: 0.0240\n",
      "Epoch [8/15], Step [500], Loss: 0.0513\n",
      "Epoch [8/15], Step [600], Loss: 0.0700\n",
      "Epoch [8/15], Step [700], Loss: 0.0592\n",
      "Epoch [8/15], Step [800], Loss: 0.0767\n",
      "Epoch [8/15], Step [900], Loss: 0.0165\n",
      "Epoch [9/15], Step [0], Loss: 0.0132\n",
      "Epoch [9/15], Step [100], Loss: 0.2041\n",
      "Epoch [9/15], Step [200], Loss: 0.0076\n",
      "Epoch [9/15], Step [300], Loss: 0.0416\n",
      "Epoch [9/15], Step [400], Loss: 0.1406\n",
      "Epoch [9/15], Step [500], Loss: 0.0151\n",
      "Epoch [9/15], Step [600], Loss: 0.1259\n",
      "Epoch [9/15], Step [700], Loss: 0.0098\n",
      "Epoch [9/15], Step [800], Loss: 0.0058\n",
      "Epoch [9/15], Step [900], Loss: 0.0440\n",
      "Epoch [10/15], Step [0], Loss: 0.0395\n",
      "Epoch [10/15], Step [100], Loss: 0.0167\n",
      "Epoch [10/15], Step [200], Loss: 0.0017\n",
      "Epoch [10/15], Step [300], Loss: 0.0437\n",
      "Epoch [10/15], Step [400], Loss: 0.0089\n",
      "Epoch [10/15], Step [500], Loss: 0.1765\n",
      "Epoch [10/15], Step [600], Loss: 0.0023\n",
      "Epoch [10/15], Step [700], Loss: 0.0057\n",
      "Epoch [10/15], Step [800], Loss: 0.0391\n",
      "Epoch [10/15], Step [900], Loss: 0.0402\n",
      "Epoch [11/15], Step [0], Loss: 0.0046\n",
      "Epoch [11/15], Step [100], Loss: 0.0143\n",
      "Epoch [11/15], Step [200], Loss: 0.0883\n",
      "Epoch [11/15], Step [300], Loss: 0.0229\n",
      "Epoch [11/15], Step [400], Loss: 0.1522\n",
      "Epoch [11/15], Step [500], Loss: 0.0068\n",
      "Epoch [11/15], Step [600], Loss: 0.0491\n",
      "Epoch [11/15], Step [700], Loss: 0.0599\n",
      "Epoch [11/15], Step [800], Loss: 0.0009\n",
      "Epoch [11/15], Step [900], Loss: 0.0599\n",
      "Epoch [12/15], Step [0], Loss: 0.0099\n",
      "Epoch [12/15], Step [100], Loss: 0.0248\n",
      "Epoch [12/15], Step [200], Loss: 0.0083\n",
      "Epoch [12/15], Step [300], Loss: 0.0846\n",
      "Epoch [12/15], Step [400], Loss: 0.0102\n",
      "Epoch [12/15], Step [500], Loss: 0.0017\n",
      "Epoch [12/15], Step [600], Loss: 0.0262\n",
      "Epoch [12/15], Step [700], Loss: 0.0217\n",
      "Epoch [12/15], Step [800], Loss: 0.0010\n",
      "Epoch [12/15], Step [900], Loss: 0.0249\n",
      "Epoch [13/15], Step [0], Loss: 0.0012\n",
      "Epoch [13/15], Step [100], Loss: 0.0177\n",
      "Epoch [13/15], Step [200], Loss: 0.0206\n",
      "Epoch [13/15], Step [300], Loss: 0.0077\n",
      "Epoch [13/15], Step [400], Loss: 0.0169\n",
      "Epoch [13/15], Step [500], Loss: 0.0115\n",
      "Epoch [13/15], Step [600], Loss: 0.0302\n",
      "Epoch [13/15], Step [700], Loss: 0.0163\n",
      "Epoch [13/15], Step [800], Loss: 0.0004\n",
      "Epoch [13/15], Step [900], Loss: 0.0121\n",
      "Epoch [14/15], Step [0], Loss: 0.0016\n",
      "Epoch [14/15], Step [100], Loss: 0.0080\n",
      "Epoch [14/15], Step [200], Loss: 0.0162\n",
      "Epoch [14/15], Step [300], Loss: 0.0304\n",
      "Epoch [14/15], Step [400], Loss: 0.0137\n",
      "Epoch [14/15], Step [500], Loss: 0.0022\n",
      "Epoch [14/15], Step [600], Loss: 0.0773\n",
      "Epoch [14/15], Step [700], Loss: 0.0229\n",
      "Epoch [14/15], Step [800], Loss: 0.0174\n",
      "Epoch [14/15], Step [900], Loss: 0.0531\n",
      "Epoch [15/15], Step [0], Loss: 0.0004\n",
      "Epoch [15/15], Step [100], Loss: 0.0089\n",
      "Epoch [15/15], Step [200], Loss: 0.0039\n",
      "Epoch [15/15], Step [300], Loss: 0.0003\n",
      "Epoch [15/15], Step [400], Loss: 0.0023\n",
      "Epoch [15/15], Step [500], Loss: 0.0200\n",
      "Epoch [15/15], Step [600], Loss: 0.0030\n",
      "Epoch [15/15], Step [700], Loss: 0.0405\n",
      "Epoch [15/15], Step [800], Loss: 0.0853\n",
      "Epoch [15/15], Step [900], Loss: 0.0109\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # put model in training mode\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Move data to device (CPU/GPU)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # 1. Reset gradients (PyTorch accumulates by default)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # 3. Compute loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # 4. Backward pass (compute gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print progress every 100 mini-batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{batch_idx}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49883e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.80%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # put model in evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # turn off gradient tracking\n",
    "    for data, target in test_loader:\n",
    "        # Move data to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass only (no backward/gradients)\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Pick class with highest score\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Count total and correct predictions\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "# Final accuracy\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
